---
title: "IIT (Integrated Information Theory)"
---

The first time I tried reading Tononi's original IIT paper, I spent three hours on the first five pages. Not because I'm stupid (I hope), but because it's written like a fucking legal contract. Every sentence is maximally compressed, every term pre-defined in a way that assumes you've already read the other papers. It's the academic equivalent of a DOS attack on your working memory.

But underneath all that noise, there's something genuinely interesting. So here's the version I wish existed when I started.

## The Problem IIT Responds To

We have a pretty decent handle on what the brain *does*. We can map which areas light up during which tasks. We can trace neural pathways, measure neurotransmitters, even predict (sometimes) what someone will decide before they know they've decided it.

But none of that tells us *why there's something it's like* to be that brain. Why doesn't all that processing happen "in the dark"? Why is there *experience* at all?

This is the "hard problem" of consciousness, and IIT is one attempt to solve it mathematically. Not by finding the right neural correlate, but by saying: **consciousness just is integrated information**.

## The Core Intuition

Imagine two scenarios:

**Scenario A:** A camera sensor. A million pixels, each doing its own thing independently. Pixel 1 detects light level, pixel 2 detects light level, etc. No pixel knows what any other pixel is doing. The camera "processes information" but there's nothing unified about it.

**Scenario B:** Your brain right now. Light hits retina → signals travel → visual cortex processes → but here's the thing: the *shape* processing neurons talk to the *color* processing neurons talk to the *motion* processing neurons. It's all bound together into one unified visual experience. You don't see shapes *over here* and colors *over there*. You see a red ball moving.

IIT says: **that binding, that integration, is consciousness**.

More precisely: a system is conscious to the degree that it is both **differentiating** (lots of possible states it could be in) and **integrated** (those states hang together as a whole, not as independent parts).

## The Math (Don't Panic)

Tononi assigns a number to this: **phi (Φ)**.

Roughly, Φ measures: *how much more the system is than the sum of its parts*.

If you cut a system in half and the two halves are basically independent, Φ is low. If cutting it destroys something essential — if the whole had properties the parts don't have — Φ is high.

Formally, it's defined as the **minimum** of **effective information** across all possible bipartitions of the system. Which sounds terrifying, but the intuition is: find the "weakest link" in the system's integration, and that's your consciousness level.

A photodiode (detects light/dark) has Φ ≈ 0.1. A human has Φ ≈ high (we don't know exactly, much higher). The internet has... actually, surprisingly low, because it's not really integrated. It's connected, but not in the right way.

## The Weird Implications

If IIT is true, some strange things follow:

1. **Consciousness is graded.** It's not "on/off." A worm is a little conscious. You are more conscious. Maybe future AI could be more conscious than humans (if it has higher Φ).

2. **Consciousness is intrinsic.** It's not about what the system *does* or how it *behaves*. It's about its internal causal structure. Two systems with identical Φ are equally conscious even if one is a brain and one is... something else entirely.

3. **Some things we think are conscious... aren't.** A feedforward neural network (the kind that does image recognition) has Φ ≈ 0. No feedback loops, no integration, no consciousness. It's a zombie. GPT-4 might be similar — lots of information, not much integration.

4. **Some things we think aren't conscious... might be.** A properly structured computer simulation? Maybe. The universe itself, if integrated in the right way? Tononi has said things that flirt with panpsychism.

## Why I'm Suspicious

Here's where I start arguing with the paper:

**The math is suspiciously flexible.** Φ depends on how you define the "elements" of the system. Neurons? Molecules? Quantum fields? Each choice gives wildly different answers. There's a principled way to choose (the "cause-effect repertoire"), but it feels like the kind of thing that could be massaged to get whatever result you want.

**It might be explaining the wrong thing.** IIT tells us *when* a system is conscious, but doesn't really explain *why* integrated information feels like something. Why doesn't it just... happen in the dark? The "explanatory gap" is still there, just moved around.

**It privileges structure over dynamics.** IIT looks at a snapshot of a system's causal structure. But consciousness feels *flowing*, *temporal*, *process-like*. A static measure might miss something essential about what it's like to be a thing that *changes*.

## Why It Still Matters

Despite all that, IIT does something important: it gives us a **research program**. Instead of just philosophizing about consciousness, we can (in theory) calculate it. We can compare two systems and say "this one is more conscious than that one." We can ask about artificial consciousness and get an actual answer, not just intuitions.

And the core intuition — that consciousness has something to do with **unity** and **distinction** — feels right in a way that purely behavioral or purely neural theories don't.

I don't think IIT is the final answer. But I think it's asking roughly the right question. The question is whether the answer is something like Φ, or whether Φ is just a symptom of something deeper.

That's what I want to figure out.
